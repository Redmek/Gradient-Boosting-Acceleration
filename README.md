# Gradient-Boosting-Acceleration

As Trevor Hastie correctly described it, Gradient Boosting is one of the best "Off-The-Shelf" algorithms in todayâ€™s world of Machine Learning. Indeed,XGBoostis a technique thatgarners great success in competitions taking place on Kaggle and other similar platforms. Ho-wever, it still remains a black box for a vast majority of its users. Gradient Boosting is aboosting technique, which consists in constructing a very accurate prediction rule by com-bining several relatively weak and imprecise rules. A surprisingly rich theory has developedaround boosting, connecting to a broad range of topics including statistics, game theory,convex optimization, and information geometry.In this paper, we will try to explain how Gradient Boosting works, and how we can accele-rate its training phase using Optimization techniques. In order to do that, we will start offby going over basic optimization definitions, followed by a presentation of different gradientdescent techniques that will, hopefully, let us achieve great results. Next, we will present thetheory behind the Classic Gradient Boosting algorithm, and talk about some of its variants.Finally, we will conclude our research on this matter by implementing our algorithms andtesting their performances on a given dataset.
